{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn import svm\n",
    "import math \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"iris-setosa.csv\")\n",
    "\n",
    "X = dataset.columns[1:3]\n",
    "X = dataset[X]\n",
    "y = dataset.columns[0]\n",
    "y = dataset[y]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X=X.astype(float)\n",
    "y=y.astype(float)\n",
    "y=np.where(y==0,-1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma):\n",
    "    # print(-linalg.norm(x-y)**2)\n",
    "    x=np.asarray(x)\n",
    "    y=np.asarray(y)\n",
    "    return np.exp((-linalg.norm(x-y)**2) / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class HYP_SVM(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, kernel=gaussian_kernel, sigma=None, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    #geometric mean\n",
    "    def score(self, y_predict, y_test):\n",
    "        print(y_predict)\n",
    "        test_min=0\n",
    "        test_max=0\n",
    "        pred_min=0\n",
    "        pred_max=0\n",
    "        y_test=np.asarray(y_test)\n",
    "        for i in range(0,len(y_test)):\n",
    "            if(y_test[i]==1):\n",
    "                test_min=test_min+1\n",
    "            else:\n",
    "                test_max=test_max+1\n",
    "        #print(\"y_test min\",test_min)       \n",
    "        #print(\"y_test max\",test_max)\n",
    "        for i in range(0,len(y_predict)):\n",
    "            if(y_predict[i]==1 and y_predict[i]==y_test[i]):\n",
    "                pred_min=pred_min+1\n",
    "            elif(y_predict[i]==-1 and y_predict[i]==y_test[i]):\n",
    "                pred_max=pred_max+1\n",
    "        #print(\"y_pred min\",pred_min)       \n",
    "        #print(\"y_pred max\",pred_max)\n",
    "        se=pred_min/test_min\n",
    "        sp=pred_max/test_max\n",
    "        #print(se,sp)\n",
    "        gm=math.sqrt(se*sp)\n",
    "        print(\"GM\",gm)    \n",
    "          \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"C\": self.C, \"sigma\": self.sigma}        \n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def m_func(self, X_train, y):\n",
    "        n_samples, n_features = X_train.shape \n",
    "        self.K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                self.K[i,j] = gaussian_kernel(X_train[i], X_train[j], self.sigma)\n",
    "               # print(K[i,j])\n",
    "        X_train=np.asarray(X_train)\n",
    "        K1 = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K1[i,j] = gaussian_kernel(X_train[i], X_train[j], self.sigma)\n",
    "               # print(K[i,j])\n",
    "        #print(K1.shape)\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        #print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a_org=a\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        self.sv_yorg=y\n",
    "        self.kernel = gaussian_kernel\n",
    "        X_train=np.asarray(X_train)\n",
    "        b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            b += self.sv_y[n]\n",
    "            b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        b /= len(self.a)\n",
    "       # print(self.a_org[1])\n",
    "        #print(self.a_org.shape,self.sv_yorg.shape,K.shape)\n",
    "        w_phi=0\n",
    "        total=0\n",
    "        for n in range(len(self.a_org)):\n",
    "            w_phi = self.a_org[n] * self.sv_yorg[n] * K1[n] \n",
    "        self.d_hyp=np.zeros(n_samples)\n",
    "        for n in range(len(self.a_org)):\n",
    "            self.d_hyp += self.sv_yorg[n]*(w_phi+b)\n",
    "        func=np.zeros((n_samples))\n",
    "        func=np.asarray(func)\n",
    "        typ=1\n",
    "        if(typ==1):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=1-(self.d_hyp[i]/(np.amax(self.d_hyp[i])+0.000001))\n",
    "        beta=0.8\n",
    "        if(typ==2):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=2/(1+beta*self.d_hyp[i])\n",
    "        r_max=103/4074\n",
    "        r_min=1\n",
    "        self.m=func[0:115]*r_min\n",
    "        #print(self.m.shape)\n",
    "        self.m=np.append(self.m,func[115:5473]*r_max)\n",
    "        #print(self.m.shape)\n",
    "        \n",
    " ##############################################################################\n",
    "\n",
    "    #prendeva come argomento anche x_test, l'ho tolto, ho aggiunto K\n",
    "    def fit(self, X_train, y):\n",
    "        \n",
    "        self.m_func(X_train, y)\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X_train.shape \n",
    "\n",
    "        # Gram matrix\n",
    "\n",
    "        #print(self.K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        #print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        for i in range(n_samples):\n",
    "            sv=np.logical_or(self.a_org <self.m, self.a_org > 1e-5)\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        #print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "        #print(self.b)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else :\n",
    "            self.w = None\n",
    "            \n",
    "        return self    \n",
    "        \n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv, self.sigma)\n",
    "                    #print(gaussian_kernel(X[i], sv, self.sigma)>0)\n",
    "                y_predict[i] = s\n",
    "                #print(y_predict[i]+self.b)\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1256e+04 -2.3472e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.2402e+04 -2.6659e+04  5e+04  1e-02  8e-14\n",
      " 2:  2.9119e+03 -4.9426e+03  8e+03  1e-03  4e-14\n",
      " 3:  4.1107e+02 -6.4678e+02  1e+03  1e-14  2e-14\n",
      " 4:  3.6536e+01 -1.0589e+02  1e+02  7e-16  9e-15\n",
      " 5: -5.1808e+00 -5.1212e+01  5e+01  2e-14  4e-15\n",
      " 6: -2.2776e+01 -4.8389e+01  3e+01  2e-14  4e-15\n",
      " 7: -2.9537e+01 -3.5336e+01  6e+00  2e-15  5e-15\n",
      " 8: -3.1239e+01 -3.2869e+01  2e+00  1e-14  4e-15\n",
      " 9: -3.1895e+01 -3.2042e+01  1e-01  3e-14  5e-15\n",
      "10: -3.1960e+01 -3.1961e+01  2e-03  1e-14  4e-15\n",
      "11: -3.1960e+01 -3.1961e+01  2e-05  1e-14  5e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1256e+04 -2.3472e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.2402e+04 -2.6659e+04  5e+04  1e-02  8e-14\n",
      " 2:  2.9119e+03 -4.9426e+03  8e+03  1e-03  4e-14\n",
      " 3:  4.1107e+02 -6.4678e+02  1e+03  1e-14  2e-14\n",
      " 4:  3.6536e+01 -1.0589e+02  1e+02  7e-16  9e-15\n",
      " 5: -5.1808e+00 -5.1212e+01  5e+01  2e-14  4e-15\n",
      " 6: -2.2776e+01 -4.8389e+01  3e+01  2e-14  4e-15\n",
      " 7: -2.9537e+01 -3.5336e+01  6e+00  2e-15  5e-15\n",
      " 8: -3.1239e+01 -3.2869e+01  2e+00  1e-14  4e-15\n",
      " 9: -3.1895e+01 -3.2042e+01  1e-01  3e-14  5e-15\n",
      "10: -3.1960e+01 -3.1961e+01  2e-03  1e-14  4e-15\n",
      "11: -3.1960e+01 -3.1961e+01  2e-05  1e-14  5e-15\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1317e+04 -2.1643e+05  4e+05  1e-01  6e-14\n",
      " 1:  1.1682e+04 -2.3778e+04  4e+04  1e-02  5e-14\n",
      " 2:  2.3028e+03 -3.4533e+03  6e+03  3e-04  4e-14\n",
      " 3:  3.0708e+02 -4.6665e+02  8e+02  3e-14  2e-14\n",
      " 4:  2.6453e+01 -7.9753e+01  1e+02  9e-15  8e-15\n",
      " 5: -6.9534e+00 -3.8292e+01  3e+01  2e-15  3e-15\n",
      " 6: -1.7792e+01 -3.5111e+01  2e+01  6e-15  2e-15\n",
      " 7: -2.3899e+01 -2.7604e+01  4e+00  5e-15  3e-15\n",
      " 8: -2.5019e+01 -2.5937e+01  9e-01  4e-15  3e-15\n",
      " 9: -2.5401e+01 -2.5435e+01  3e-02  1e-14  3e-15\n",
      "10: -2.5416e+01 -2.5417e+01  3e-04  3e-15  3e-15\n",
      "11: -2.5416e+01 -2.5416e+01  3e-06  7e-15  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1317e+04 -2.1643e+05  4e+05  1e-01  6e-14\n",
      " 1:  1.1682e+04 -2.3778e+04  4e+04  1e-02  5e-14\n",
      " 2:  2.3028e+03 -3.4533e+03  6e+03  3e-04  4e-14\n",
      " 3:  3.0708e+02 -4.6665e+02  8e+02  3e-14  2e-14\n",
      " 4:  2.6453e+01 -7.9753e+01  1e+02  9e-15  8e-15\n",
      " 5: -6.9534e+00 -3.8292e+01  3e+01  2e-15  3e-15\n",
      " 6: -1.7792e+01 -3.5111e+01  2e+01  6e-15  2e-15\n",
      " 7: -2.3899e+01 -2.7604e+01  4e+00  5e-15  3e-15\n",
      " 8: -2.5019e+01 -2.5937e+01  9e-01  4e-15  3e-15\n",
      " 9: -2.5401e+01 -2.5435e+01  3e-02  1e-14  3e-15\n",
      "10: -2.5416e+01 -2.5417e+01  3e-04  3e-15  3e-15\n",
      "11: -2.5416e+01 -2.5416e+01  3e-06  7e-15  3e-15\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1648e+04 -1.9772e+05  3e+05  1e-01  5e-14\n",
      " 1:  1.0701e+04 -2.0844e+04  4e+04  8e-03  5e-14\n",
      " 2:  2.1246e+03 -3.3268e+03  6e+03  3e-04  3e-14\n",
      " 3:  2.8284e+02 -4.4435e+02  7e+02  1e-14  2e-14\n",
      " 4:  2.2702e+01 -7.9372e+01  1e+02  1e-14  7e-15\n",
      " 5: -1.3315e+01 -2.9916e+01  2e+01  3e-15  4e-15\n",
      " 6: -1.8022e+01 -2.4977e+01  7e+00  5e-15  3e-15\n",
      " 7: -2.1155e+01 -2.3936e+01  3e+00  2e-14  3e-15\n",
      " 8: -2.2224e+01 -2.2423e+01  2e-01  9e-15  3e-15\n",
      " 9: -2.2311e+01 -2.2313e+01  3e-03  7e-15  3e-15\n",
      "10: -2.2312e+01 -2.2312e+01  3e-05  2e-14  3e-15\n",
      "11: -2.2312e+01 -2.2312e+01  3e-07  2e-14  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1648e+04 -1.9772e+05  3e+05  1e-01  5e-14\n",
      " 1:  1.0701e+04 -2.0844e+04  4e+04  8e-03  5e-14\n",
      " 2:  2.1246e+03 -3.3268e+03  6e+03  3e-04  3e-14\n",
      " 3:  2.8284e+02 -4.4435e+02  7e+02  1e-14  2e-14\n",
      " 4:  2.2702e+01 -7.9372e+01  1e+02  1e-14  7e-15\n",
      " 5: -1.3315e+01 -2.9916e+01  2e+01  3e-15  4e-15\n",
      " 6: -1.8022e+01 -2.4977e+01  7e+00  5e-15  3e-15\n",
      " 7: -2.1155e+01 -2.3936e+01  3e+00  2e-14  3e-15\n",
      " 8: -2.2224e+01 -2.2423e+01  2e-01  9e-15  3e-15\n",
      " 9: -2.2311e+01 -2.2313e+01  3e-03  7e-15  3e-15\n",
      "10: -2.2312e+01 -2.2312e+01  3e-05  2e-14  3e-15\n",
      "11: -2.2312e+01 -2.2312e+01  3e-07  2e-14  3e-15\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1950e+04 -2.3253e+05  4e+05  1e-01  4e-14\n",
      " 1:  1.2711e+04 -2.6686e+04  5e+04  1e-02  5e-14\n",
      " 2:  2.3770e+03 -3.1935e+03  6e+03  2e-13  3e-14\n",
      " 3:  3.1261e+02 -4.5271e+02  8e+02  4e-14  2e-14\n",
      " 4:  3.0070e+01 -7.6906e+01  1e+02  3e-14  8e-15\n",
      " 5: -6.9102e-01 -3.3356e+01  3e+01  9e-15  3e-15\n",
      " 6: -1.4787e+01 -3.3637e+01  2e+01  1e-15  3e-15\n",
      " 7: -2.1562e+01 -2.4339e+01  3e+00  1e-15  3e-15\n",
      " 8: -2.2756e+01 -2.3016e+01  3e-01  9e-15  4e-15\n",
      " 9: -2.2883e+01 -2.2892e+01  9e-03  1e-14  4e-15\n",
      "10: -2.2888e+01 -2.2889e+01  4e-04  2e-15  3e-15\n",
      "11: -2.2889e+01 -2.2889e+01  4e-06  4e-15  4e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1950e+04 -2.3253e+05  4e+05  1e-01  4e-14\n",
      " 1:  1.2711e+04 -2.6686e+04  5e+04  1e-02  5e-14\n",
      " 2:  2.3770e+03 -3.1935e+03  6e+03  2e-13  3e-14\n",
      " 3:  3.1261e+02 -4.5271e+02  8e+02  4e-14  2e-14\n",
      " 4:  3.0070e+01 -7.6906e+01  1e+02  3e-14  8e-15\n",
      " 5: -6.9102e-01 -3.3356e+01  3e+01  9e-15  3e-15\n",
      " 6: -1.4787e+01 -3.3637e+01  2e+01  1e-15  3e-15\n",
      " 7: -2.1562e+01 -2.4339e+01  3e+00  1e-15  3e-15\n",
      " 8: -2.2756e+01 -2.3016e+01  3e-01  9e-15  4e-15\n",
      " 9: -2.2883e+01 -2.2892e+01  9e-03  1e-14  4e-15\n",
      "10: -2.2888e+01 -2.2889e+01  4e-04  2e-15  3e-15\n",
      "11: -2.2889e+01 -2.2889e+01  4e-06  4e-15  4e-15\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0718e+04 -2.2352e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.1815e+04 -2.5269e+04  4e+04  1e-02  9e-14\n",
      " 2:  2.6600e+03 -4.3881e+03  7e+03  8e-04  3e-14\n",
      " 3:  3.6785e+02 -5.7314e+02  9e+02  4e-14  2e-14\n",
      " 4:  3.0878e+01 -9.6663e+01  1e+02  6e-15  9e-15\n",
      " 5: -5.1539e+00 -5.0650e+01  5e+01  3e-14  4e-15\n",
      " 6: -2.3820e+01 -4.6712e+01  2e+01  2e-14  4e-15\n",
      " 7: -3.0360e+01 -3.4153e+01  4e+00  6e-16  5e-15\n",
      " 8: -3.1495e+01 -3.2683e+01  1e+00  1e-15  5e-15\n",
      " 9: -3.2001e+01 -3.2051e+01  5e-02  6e-16  4e-15\n",
      "10: -3.2024e+01 -3.2024e+01  5e-04  1e-14  4e-15\n",
      "11: -3.2024e+01 -3.2024e+01  5e-06  3e-14  5e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0718e+04 -2.2352e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.1815e+04 -2.5269e+04  4e+04  1e-02  9e-14\n",
      " 2:  2.6600e+03 -4.3881e+03  7e+03  8e-04  3e-14\n",
      " 3:  3.6785e+02 -5.7314e+02  9e+02  4e-14  2e-14\n",
      " 4:  3.0878e+01 -9.6663e+01  1e+02  6e-15  9e-15\n",
      " 5: -5.1539e+00 -5.0650e+01  5e+01  3e-14  4e-15\n",
      " 6: -2.3820e+01 -4.6712e+01  2e+01  2e-14  4e-15\n",
      " 7: -3.0360e+01 -3.4153e+01  4e+00  6e-16  5e-15\n",
      " 8: -3.1495e+01 -3.2683e+01  1e+00  1e-15  5e-15\n",
      " 9: -3.2001e+01 -3.2051e+01  5e-02  6e-16  4e-15\n",
      "10: -3.2024e+01 -3.2024e+01  5e-04  1e-14  4e-15\n",
      "11: -3.2024e+01 -3.2024e+01  5e-06  3e-14  5e-15\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def hyp_svm():\n",
    "        C_vals = [1e-2, 1e-1, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "        sigma = [9e-2, 9e-1, 9e+1, 9e+2, 9e+3, 9e+4]\n",
    "        combs = list(itertools.product(C_vals, sigma))\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            typ = 2            \n",
    "            clf = HYP_SVM(C=100.0, kernel=gaussian_kernel, sigma=0.9)    \n",
    "    \n",
    "            #clf.m_func(X_train, y_train)\n",
    "                \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_predict = clf.predict(X_test)\n",
    "\n",
    "            clf.score(y_predict,y_test)\n",
    "            #print(y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "            \n",
    "            \n",
    "    hyp_svm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_svm = svm.SVC(kernel='rbf', gamma=0.001, C=100)\n",
    "#clf_svm.fit(X_train, y_train)\n",
    "#y_pred_svm = clf_svm.predict(X_test) \n",
    "#acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "#print (\"Overall RBF KERNEL SVM accuracy: \",acc_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = [1e-2, 1e-1, 1, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "sigma = [9e-2, 9e-1, 9, 9e+1, 9e+2, 9e+3, 9e+4]\n",
    "#kernels = [\"linear_kernel\", \"polynomial_kernel\", \"gaussian_kernel\"]\n",
    "parameters = {'C': C_vals, 'sigma': sigma}\n",
    "\n",
    "model = HYP_SVM(C=100, kernel=gaussian_kernel, sigma=0.9)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y)\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5496e+01 -6.3071e+00  4e+02  2e+01  3e-16\n",
      " 1: -5.3696e+00 -2.1583e+00  3e+01  2e+00  4e-16\n",
      " 2: -6.4709e-01 -1.6454e+00  1e+00  2e-02  1e-15\n",
      " 3: -5.9140e-01 -6.5369e-01  6e-02  2e-17  7e-16\n",
      " 4: -5.9486e-01 -5.9979e-01  5e-03  2e-17  3e-16\n",
      " 5: -5.9547e-01 -5.9608e-01  6e-04  2e-17  3e-16\n",
      " 6: -5.9558e-01 -5.9565e-01  7e-05  2e-17  4e-16\n",
      " 7: -5.9559e-01 -5.9560e-01  7e-06  2e-17  4e-16\n",
      " 8: -5.9559e-01 -5.9559e-01  3e-07  2e-17  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5496e+01 -6.3071e+00  4e+02  2e+01  3e-16\n",
      " 1: -5.3696e+00 -2.1583e+00  3e+01  2e+00  4e-16\n",
      " 2: -6.4709e-01 -1.6454e+00  1e+00  2e-02  1e-15\n",
      " 3: -5.9140e-01 -6.5369e-01  6e-02  2e-17  7e-16\n",
      " 4: -5.9486e-01 -5.9979e-01  5e-03  2e-17  3e-16\n",
      " 5: -5.9547e-01 -5.9608e-01  6e-04  2e-17  3e-16\n",
      " 6: -5.9558e-01 -5.9565e-01  7e-05  2e-17  4e-16\n",
      " 7: -5.9559e-01 -5.9560e-01  7e-06  2e-17  4e-16\n",
      " 8: -5.9559e-01 -5.9559e-01  3e-07  2e-17  3e-16\n",
      "Optimal solution found.\n",
      "[ 1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1.  1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f2620385bfc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clf.score(xTest, yTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#clf.best_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                       *args, **kwargs)\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-982c94d6b285>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, y_test, y_predict)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mtest_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_min\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "grid_result = clf.fit(X=xTrain, y=yTrain)\n",
    "\n",
    "#clf.score(xTest, yTest)\n",
    "#clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=gaussian_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "    def fit(self, X, y):\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X.shape\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = gaussian_kernel(X[i], X[j])\n",
    "               # print(K[i,j])\n",
    "        print(K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "       # print(a)\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "                #print(self.w)\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "              #  print(y_predict[i])\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def normal_svm():\n",
    "        \n",
    "        clf = SVM(C=100.0)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)    \n",
    "            y_predict = clf.predict(X_test)\n",
    "            gm(y_predict,y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "    normal_svm()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
