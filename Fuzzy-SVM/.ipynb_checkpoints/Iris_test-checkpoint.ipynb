{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn import svm\n",
    "import math \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "dataset=pd.read_csv(\"iris-setosa.csv\")\n",
    "\n",
    "X = dataset.columns[1:3]\n",
    "X = dataset[X]\n",
    "y = dataset.columns[0]\n",
    "y = dataset[y]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X=X.astype(float)\n",
    "y=y.astype(float)\n",
    "y=np.where(y==0,-1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=90.0):\n",
    "    # print(-linalg.norm(x-y)**2)\n",
    "    x=np.asarray(x)\n",
    "    y=np.asarray(y)\n",
    "    return np.exp((-linalg.norm(x-y)**2) / (2 * (sigma ** 2)))\n",
    "\n",
    "#geometric mean\n",
    "def score(y_predict,y_test):\n",
    "    test_min=0\n",
    "    test_max=0\n",
    "    pred_min=0\n",
    "    pred_max=0\n",
    "    y_test=np.asarray(y_test)\n",
    "    for i in range(0,len(y_test)):\n",
    "        if(y_test[i]==1):\n",
    "            test_min=test_min+1\n",
    "        else:\n",
    "            test_max=test_max+1\n",
    "    print(\"y_test min\",test_min)       \n",
    "    print(\"y_test max\",test_max)\n",
    "    for i in range(0,len(y_predict)):\n",
    "        if(y_predict[i]==1 and y_predict[i]==y_test[i]):\n",
    "            pred_min=pred_min+1\n",
    "        elif(y_predict[i]==-1 and y_predict[i]==y_test[i]):\n",
    "            pred_max=pred_max+1\n",
    "    print(\"y_pred min\",pred_min)       \n",
    "    print(\"y_pred max\",pred_max)\n",
    "    se=pred_min/test_min\n",
    "    sp=pred_max/test_max\n",
    "    print(se,sp)\n",
    "    gm=math.sqrt(se*sp)\n",
    "    print(\"GM\",gm)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class HYP_SVM(object):\n",
    "\n",
    "    #geometric mean\n",
    "    def score(y_predict,y_test):\n",
    "        test_min=0\n",
    "        test_max=0\n",
    "        pred_min=0\n",
    "        pred_max=0\n",
    "        y_test=np.asarray(y_test)\n",
    "        for i in range(0,len(y_test)):\n",
    "            if(y_test[i]==1):\n",
    "                test_min=test_min+1\n",
    "            else:\n",
    "                test_max=test_max+1\n",
    "        print(\"y_test min\",test_min)       \n",
    "        print(\"y_test max\",test_max)\n",
    "        for i in range(0,len(y_predict)):\n",
    "            if(y_predict[i]==1 and y_predict[i]==y_test[i]):\n",
    "                pred_min=pred_min+1\n",
    "            elif(y_predict[i]==-1 and y_predict[i]==y_test[i]):\n",
    "                pred_max=pred_max+1\n",
    "        print(\"y_pred min\",pred_min)       \n",
    "        print(\"y_pred max\",pred_max)\n",
    "        se=pred_min/test_min\n",
    "        sp=pred_max/test_max\n",
    "        print(se,sp)\n",
    "        gm=math.sqrt(se*sp)\n",
    "        print(\"GM\",gm)    \n",
    "    \n",
    "    def __init__(self, kernel=gaussian_kernel, sigma=None, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"C\": self.C, \"sigma\": self.sigma, \"kernel\": self.kernel}        \n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def m_func(self, X_train,X_test, y):\n",
    "        n_samples, n_features = X_train.shape \n",
    "        nt_samples, nt_features= X_test.shape\n",
    "        self.K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                self.K[i,j] = gaussian_kernel(X_train[i], X_train[j], self.sigma)\n",
    "               # print(K[i,j])\n",
    "        X_train=np.asarray(X_train)\n",
    "        X_test=np.asarray(X_test)\n",
    "        K1 = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K1[i,j] = gaussian_kernel(X_train[i], X_train[j], self.sigma)\n",
    "               # print(K[i,j])\n",
    "        print(K1.shape)\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a_org=a\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        self.sv_yorg=y\n",
    "        self.kernel = gaussian_kernel\n",
    "        X_train=np.asarray(X_train)\n",
    "        b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            b += self.sv_y[n]\n",
    "            b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        b /= len(self.a)\n",
    "       # print(self.a_org[1])\n",
    "        #print(self.a_org.shape,self.sv_yorg.shape,K.shape)\n",
    "        w_phi=0\n",
    "        total=0\n",
    "        for n in range(len(self.a_org)):\n",
    "            w_phi = self.a_org[n] * self.sv_yorg[n] * K1[n] \n",
    "        self.d_hyp=np.zeros(n_samples)\n",
    "        for n in range(len(self.a_org)):\n",
    "            self.d_hyp += self.sv_yorg[n]*(w_phi+b)\n",
    "        func=np.zeros((n_samples))\n",
    "        func=np.asarray(func)\n",
    "        typ=1\n",
    "        if(typ==1):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=1-(self.d_hyp[i]/(np.amax(self.d_hyp[i])+0.000001))\n",
    "        beta=0.8\n",
    "        if(typ==2):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=2/(1+beta*self.d_hyp[i])\n",
    "        r_max=103/4074\n",
    "        r_min=1\n",
    "        self.m=func[0:115]*r_min\n",
    "        print(self.m.shape)\n",
    "        self.m=np.append(self.m,func[115:5473]*r_max)\n",
    "        print(self.m.shape)\n",
    "        \n",
    " ##############################################################################\n",
    "\n",
    "    #prendeva come argomento anche x_test, l'ho tolto, ho aggiunto K\n",
    "    def fit(self, X_train, y):\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X_train.shape \n",
    "        #nt_samples, nt_features = X_test.shape\n",
    "        # Gram matrix\n",
    "\n",
    "        print(self.K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        for i in range(n_samples):\n",
    "            sv=np.logical_or(self.a_org <self.m, self.a_org > 1e-5)\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        #print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "        print(self.b)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else :\n",
    "            self.w = None        \n",
    "        \n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv, self.sigma)\n",
    "                    #print(gaussian_kernel(X[i], sv, self.sigma)>0)\n",
    "                y_predict[i] = s\n",
    "                #print(y_predict[i]+self.b)\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1725e+04 -2.2554e+05  4e+05  1e-01  4e-14\n",
      " 1:  1.2326e+04 -2.6641e+04  5e+04  1e-02  5e-14\n",
      " 2:  2.3237e+03 -3.0579e+03  5e+03  1e-13  3e-14\n",
      " 3:  3.0650e+02 -4.4096e+02  7e+02  3e-14  2e-14\n",
      " 4:  3.0290e+01 -7.4418e+01  1e+02  8e-15  8e-15\n",
      " 5: -1.0084e+00 -2.7268e+01  3e+01  5e-15  4e-15\n",
      " 6: -1.3198e+01 -2.2240e+01  9e+00  6e-15  3e-15\n",
      " 7: -1.6838e+01 -1.8960e+01  2e+00  2e-15  3e-15\n",
      " 8: -1.7964e+01 -1.8194e+01  2e-01  1e-14  3e-15\n",
      " 9: -1.8056e+01 -1.8082e+01  3e-02  1e-14  3e-15\n",
      "10: -1.8068e+01 -1.8068e+01  3e-04  8e-15  3e-15\n",
      "11: -1.8068e+01 -1.8068e+01  3e-06  2e-14  3e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1725e+04 -2.2554e+05  4e+05  1e-01  4e-14\n",
      " 1:  1.2326e+04 -2.6641e+04  5e+04  1e-02  5e-14\n",
      " 2:  2.3237e+03 -3.0579e+03  5e+03  1e-13  3e-14\n",
      " 3:  3.0650e+02 -4.4096e+02  7e+02  3e-14  2e-14\n",
      " 4:  3.0290e+01 -7.4418e+01  1e+02  8e-15  8e-15\n",
      " 5: -1.0084e+00 -2.7268e+01  3e+01  5e-15  4e-15\n",
      " 6: -1.3198e+01 -2.2240e+01  9e+00  6e-15  3e-15\n",
      " 7: -1.6838e+01 -1.8960e+01  2e+00  2e-15  3e-15\n",
      " 8: -1.7964e+01 -1.8194e+01  2e-01  1e-14  3e-15\n",
      " 9: -1.8056e+01 -1.8082e+01  3e-02  1e-14  3e-15\n",
      "10: -1.8068e+01 -1.8068e+01  3e-04  8e-15  3e-15\n",
      "11: -1.8068e+01 -1.8068e+01  3e-06  2e-14  3e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "0.2092586245135339\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1906e+04 -2.5275e+05  4e+05  2e-01  5e-14\n",
      " 1:  1.3344e+04 -2.8203e+04  5e+04  1e-02  7e-14\n",
      " 2:  3.1341e+03 -5.2767e+03  9e+03  1e-03  4e-14\n",
      " 3:  4.4458e+02 -6.9140e+02  1e+03  7e-14  2e-14\n",
      " 4:  4.1173e+01 -1.1211e+02  2e+02  1e-15  1e-14\n",
      " 5: -5.5192e+00 -4.9926e+01  4e+01  2e-15  4e-15\n",
      " 6: -2.1213e+01 -5.0106e+01  3e+01  9e-15  4e-15\n",
      " 7: -2.9473e+01 -3.5026e+01  6e+00  2e-14  4e-15\n",
      " 8: -3.1508e+01 -3.2427e+01  9e-01  5e-16  5e-15\n",
      " 9: -3.1875e+01 -3.2067e+01  2e-01  8e-16  5e-15\n",
      "10: -3.1959e+01 -3.1961e+01  2e-03  5e-15  4e-15\n",
      "11: -3.1960e+01 -3.1960e+01  2e-05  2e-14  4e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1906e+04 -2.5275e+05  4e+05  2e-01  5e-14\n",
      " 1:  1.3344e+04 -2.8203e+04  5e+04  1e-02  7e-14\n",
      " 2:  3.1341e+03 -5.2767e+03  9e+03  1e-03  4e-14\n",
      " 3:  4.4458e+02 -6.9140e+02  1e+03  7e-14  2e-14\n",
      " 4:  4.1173e+01 -1.1211e+02  2e+02  1e-15  1e-14\n",
      " 5: -5.5192e+00 -4.9926e+01  4e+01  2e-15  4e-15\n",
      " 6: -2.1213e+01 -5.0106e+01  3e+01  9e-15  4e-15\n",
      " 7: -2.9473e+01 -3.5026e+01  6e+00  2e-14  4e-15\n",
      " 8: -3.1508e+01 -3.2427e+01  9e-01  5e-16  5e-15\n",
      " 9: -3.1875e+01 -3.2067e+01  2e-01  8e-16  5e-15\n",
      "10: -3.1959e+01 -3.1961e+01  2e-03  5e-15  4e-15\n",
      "11: -3.1960e+01 -3.1960e+01  2e-05  2e-14  4e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "0.1410029979198533\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0594e+04 -2.1313e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.1279e+04 -2.3552e+04  4e+04  1e-02  5e-14\n",
      " 2:  2.4416e+03 -3.9341e+03  7e+03  6e-04  3e-14\n",
      " 3:  3.3194e+02 -5.1798e+02  8e+02  6e-14  2e-14\n",
      " 4:  2.5990e+01 -8.9195e+01  1e+02  2e-14  8e-15\n",
      " 5: -5.8331e+00 -4.9933e+01  4e+01  9e-15  4e-15\n",
      " 6: -2.4269e+01 -4.4702e+01  2e+01  1e-14  4e-15\n",
      " 7: -3.0213e+01 -3.4365e+01  4e+00  5e-15  4e-15\n",
      " 8: -3.1487e+01 -3.2693e+01  1e+00  3e-16  4e-15\n",
      " 9: -3.2006e+01 -3.2044e+01  4e-02  1e-14  5e-15\n",
      "10: -3.2024e+01 -3.2024e+01  4e-04  1e-14  5e-15\n",
      "11: -3.2024e+01 -3.2024e+01  4e-06  6e-16  4e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0594e+04 -2.1313e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.1279e+04 -2.3552e+04  4e+04  1e-02  5e-14\n",
      " 2:  2.4416e+03 -3.9341e+03  7e+03  6e-04  3e-14\n",
      " 3:  3.3194e+02 -5.1798e+02  8e+02  6e-14  2e-14\n",
      " 4:  2.5990e+01 -8.9195e+01  1e+02  2e-14  8e-15\n",
      " 5: -5.8331e+00 -4.9933e+01  4e+01  9e-15  4e-15\n",
      " 6: -2.4269e+01 -4.4702e+01  2e+01  1e-14  4e-15\n",
      " 7: -3.0213e+01 -3.4365e+01  4e+00  5e-15  4e-15\n",
      " 8: -3.1487e+01 -3.2693e+01  1e+00  3e-16  4e-15\n",
      " 9: -3.2006e+01 -3.2044e+01  4e-02  1e-14  5e-15\n",
      "10: -3.2024e+01 -3.2024e+01  4e-04  1e-14  5e-15\n",
      "11: -3.2024e+01 -3.2024e+01  4e-06  6e-16  4e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "0.17530465503702936\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1445e+04 -2.1441e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.1685e+04 -2.3963e+04  4e+04  1e-02  6e-14\n",
      " 2:  2.3988e+03 -3.7616e+03  6e+03  4e-04  3e-14\n",
      " 3:  3.2426e+02 -4.9582e+02  8e+02  3e-14  2e-14\n",
      " 4:  2.8814e+01 -8.3612e+01  1e+02  2e-14  9e-15\n",
      " 5: -7.3364e+00 -3.8329e+01  3e+01  6e-15  3e-15\n",
      " 6: -1.8541e+01 -3.2212e+01  1e+01  2e-15  3e-15\n",
      " 7: -2.1885e+01 -2.6435e+01  5e+00  8e-15  3e-15\n",
      " 8: -2.3187e+01 -2.4382e+01  1e+00  5e-15  2e-15\n",
      " 9: -2.3651e+01 -2.3737e+01  9e-02  7e-15  3e-15\n",
      "10: -2.3688e+01 -2.3689e+01  9e-04  6e-16  3e-15\n",
      "11: -2.3689e+01 -2.3689e+01  9e-06  4e-15  3e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1445e+04 -2.1441e+05  4e+05  1e-01  5e-14\n",
      " 1:  1.1685e+04 -2.3963e+04  4e+04  1e-02  6e-14\n",
      " 2:  2.3988e+03 -3.7616e+03  6e+03  4e-04  3e-14\n",
      " 3:  3.2426e+02 -4.9582e+02  8e+02  3e-14  2e-14\n",
      " 4:  2.8814e+01 -8.3612e+01  1e+02  2e-14  9e-15\n",
      " 5: -7.3364e+00 -3.8329e+01  3e+01  6e-15  3e-15\n",
      " 6: -1.8541e+01 -3.2212e+01  1e+01  2e-15  3e-15\n",
      " 7: -2.1885e+01 -2.6435e+01  5e+00  8e-15  3e-15\n",
      " 8: -2.3187e+01 -2.4382e+01  1e+00  5e-15  2e-15\n",
      " 9: -2.3651e+01 -2.3737e+01  9e-02  7e-15  3e-15\n",
      "10: -2.3688e+01 -2.3689e+01  9e-04  6e-16  3e-15\n",
      "11: -2.3689e+01 -2.3689e+01  9e-06  4e-15  3e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "0.08013091731247962\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1151e+04 -1.9431e+05  3e+05  1e-01  5e-14\n",
      " 1:  1.0387e+04 -2.1147e+04  4e+04  8e-03  8e-14\n",
      " 2:  2.2272e+03 -3.7046e+03  6e+03  6e-04  4e-14\n",
      " 3:  3.0086e+02 -4.8653e+02  8e+02  5e-14  2e-14\n",
      " 4:  2.2279e+01 -8.3617e+01  1e+02  9e-15  8e-15\n",
      " 5: -6.0929e+00 -4.9036e+01  4e+01  3e-15  4e-15\n",
      " 6: -2.2816e+01 -4.4104e+01  2e+01  1e-14  3e-15\n",
      " 7: -2.9545e+01 -3.4891e+01  5e+00  5e-15  5e-15\n",
      " 8: -3.1153e+01 -3.2576e+01  1e+00  1e-15  4e-15\n",
      " 9: -3.1748e+01 -3.1833e+01  9e-02  3e-15  5e-15\n",
      "10: -3.1791e+01 -3.1792e+01  1e-03  1e-14  7e-15\n",
      "11: -3.1792e+01 -3.1792e+01  1e-05  1e-14  5e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1151e+04 -1.9431e+05  3e+05  1e-01  5e-14\n",
      " 1:  1.0387e+04 -2.1147e+04  4e+04  8e-03  8e-14\n",
      " 2:  2.2272e+03 -3.7046e+03  6e+03  6e-04  4e-14\n",
      " 3:  3.0086e+02 -4.8653e+02  8e+02  5e-14  2e-14\n",
      " 4:  2.2279e+01 -8.3617e+01  1e+02  9e-15  8e-15\n",
      " 5: -6.0929e+00 -4.9036e+01  4e+01  3e-15  4e-15\n",
      " 6: -2.2816e+01 -4.4104e+01  2e+01  1e-14  3e-15\n",
      " 7: -2.9545e+01 -3.4891e+01  5e+00  5e-15  5e-15\n",
      " 8: -3.1153e+01 -3.2576e+01  1e+00  1e-15  4e-15\n",
      " 9: -3.1748e+01 -3.1833e+01  9e-02  3e-15  5e-15\n",
      "10: -3.1791e+01 -3.1792e+01  1e-03  1e-14  7e-15\n",
      "11: -3.1792e+01 -3.1792e+01  1e-05  1e-14  5e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "0.1749275808602378\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def hyp_svm():\n",
    "        C_vals = [1e-2, 1e-1, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "        sigma = [9e-2, 9e-1, 9e+1, 9e+2, 9e+3, 9e+4]\n",
    "        combs = list(itertools.product(C_vals, sigma))\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            typ = 2            \n",
    "            clf = HYP_SVM(C=100, kernel=gaussian_kernel, sigma=0.9)    \n",
    "    \n",
    "            clf.m_func(X_train,X_test,y_train)\n",
    "                \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_predict = clf.predict(X_test)\n",
    "            \n",
    "            score(y_predict,y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "            \n",
    "    hyp_svm()\n",
    "    #print(y_predict[i]+self.b) nella funzione project, poi considera i positivi come casi di 1 e i negativi come -1\n",
    "    #Gaussian_kernel potrebbe calcolare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When assigning fuzzy memberships for training examples\n",
    "in the FSVM-CIL method, in order to reflect the class imbalance, we assign r + = 1, and r − = r, where r is the minority-to-majority class ratio. \n",
    "According to this assignment of values, a positive-\n",
    "class example can take a membership value in the [0, 1] interval,\n",
    "while a negative-class example can take a membership value in\n",
    "the [0, r] interval, where r < 1.\n",
    "\n",
    "[...]\n",
    "\n",
    "selected the following ranges for parameters: log 2 C = {1, 2, . . . , 15}, and\n",
    "log 2 γ = {−15, −14, . . . , −1}. First, we conducted a coarse\n",
    "grid-parameter-search to find the optimal values for log 2 C and\n",
    "log 2 γ over the aforementioned ranges of values. We evalu-\n",
    "ated the performance of a model trained on each parameter\n",
    "pair (log 2 C, log 2 γ) by using five-fold cross-validation results\n",
    "on the training dataset. After finding the optimal values for\n",
    "the parameters, say ( C̄,γ̄), again, a narrow grid-parameter-\n",
    "search was conducted over the ranges log 2 C = {c̄ − 0.75, c̄ −\n",
    "0.5, . . . , c̄ + 0.75}, and log 2 γ = {γ̄ − 0.75, γ̄ − 0.5, . . . , γ̄ +\n",
    "0.75}. Finally, after finding the optimal values for (log 2 C,\n",
    "log 2 γ), a new SVM model was trained by using the complete\n",
    "training dataset on these parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(kernel='rbf', gamma=0.001, C=100)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_pred_svm = clf_svm.predict(X_test) \n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print (\"Overall RBF KERNEL SVM accuracy: \",acc_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1322e+04 -1.9998e+05  3e+05  1e-01  3e-14\n",
      " 1:  1.0630e+04 -2.1027e+04  4e+04  8e-03  5e-14\n",
      " 2:  2.3022e+03 -3.8752e+03  6e+03  6e-04  2e-14\n",
      " 3:  3.1205e+02 -5.0965e+02  8e+02  8e-15  2e-14\n",
      " 4:  2.3312e+01 -8.6722e+01  1e+02  2e-16  7e-15\n",
      " 5: -6.9139e+00 -4.9955e+01  4e+01  7e-15  3e-15\n",
      " 6: -2.4522e+01 -4.4938e+01  2e+01  7e-15  3e-15\n",
      " 7: -3.0007e+01 -3.4661e+01  5e+00  1e-15  3e-15\n",
      " 8: -3.1374e+01 -3.2711e+01  1e+00  5e-15  4e-15\n",
      " 9: -3.1938e+01 -3.1988e+01  5e-02  6e-15  4e-15\n",
      "10: -3.1960e+01 -3.1961e+01  5e-04  3e-15  4e-15\n",
      "11: -3.1960e+01 -3.1961e+01  5e-06  1e-14  4e-15\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessia/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "AttributeError: 'HYP_SVM' object has no attribute 'K'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HYP_SVM' object has no attribute 'K'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-93042925f074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#clf.score(xTest, yTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#clf.best_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e558ad21624d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Gram matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HYP_SVM' object has no attribute 'K'"
     ]
    }
   ],
   "source": [
    "C_vals = [1e-2, 1e-1, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "sigma = [9e-2, 9e-1, 9e+1, 9e+2, 9e+3, 9e+4]\n",
    "kernels = [\"linear_kernel\", \"polynomial_kernel\", \"gaussian_kernel\"]\n",
    "parameters = {'C': C_vals, 'sigma': sigma, 'kernel' : kernels }\n",
    "\n",
    "model = HYP_SVM(C=100, kernel=gaussian_kernel, sigma=0.9)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X,y,  test_size=30)\n",
    "model.m_func(xTrain,xTest,yTrain)\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv = 5, return_train_score=True)\n",
    "\n",
    "clf.fit(xTrain, yTrain)\n",
    "#clf.score(xTest, yTest)\n",
    "#clf.best_params_\n",
    "\n",
    "#best_model = FuzzyMMC(sensitivity=0.5, exp_bound=0.60, animate=False)\n",
    "#best_model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=gaussian_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "    def fit(self, X, y):\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X.shape\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = gaussian_kernel(X[i], X[j])\n",
    "               # print(K[i,j])\n",
    "        print(K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "       # print(a)\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "                #print(self.w)\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "              #  print(y_predict[i])\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def normal_svm():\n",
    "        \n",
    "        clf = SVM(C=100.0)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)    \n",
    "            y_predict = clf.predict(X_test)\n",
    "            gm(y_predict,y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "    normal_svm()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
