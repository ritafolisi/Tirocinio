{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn import svm\n",
    "import math \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"iris-setosa.csv\")\n",
    "\n",
    "X = dataset.columns[3:5]\n",
    "X = dataset[X]\n",
    "y = dataset.columns[0]\n",
    "y = dataset[y]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X=X.astype(float)\n",
    "y=y.astype(float)\n",
    "y=np.where(y==0,-1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma):\n",
    "    # print(-linalg.norm(x-y)**2)\n",
    "    x=np.asarray(x)\n",
    "    y=np.asarray(y)\n",
    "    return np.exp((-linalg.norm(x-y)**2) / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class HYP_SVM(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, kernel=gaussian_kernel, sigma=None, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    #geometric mean\n",
    "    def score(self, y_predict,y_test):\n",
    "        print(y_predict)\n",
    "        test_min=0\n",
    "        test_max=0\n",
    "        pred_min=0\n",
    "        pred_max=0\n",
    "        y_test=np.asarray(y_test)\n",
    "        for i in range(0,len(y_test)):\n",
    "            if(y_test[i]==1):\n",
    "                test_min=test_min+1\n",
    "            else:\n",
    "                test_max=test_max+1\n",
    "        #print(\"y_test min\",test_min)       \n",
    "        #print(\"y_test max\",test_max)\n",
    "        for i in range(0,len(y_predict)):\n",
    "            if(y_predict[i]==1 and y_predict[i]==y_test[i]):\n",
    "                pred_min=pred_min+1\n",
    "            elif(y_predict[i]==-1 and y_predict[i]==y_test[i]):\n",
    "                pred_max=pred_max+1\n",
    "        #print(\"y_pred min\",pred_min)       \n",
    "        #print(\"y_pred max\",pred_max)\n",
    "        se=pred_min/test_min\n",
    "        sp=pred_max/test_max\n",
    "        #print(se,sp)\n",
    "        gm=math.sqrt(se*sp)\n",
    "        print(\"GM\",gm)    \n",
    "          \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"C\": self.C, \"sigma\": self.sigma}        \n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def m_func(self, X_train, y):\n",
    "        n_samples, n_features = X_train.shape \n",
    "        self.K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                self.K[i,j] = gaussian_kernel(X_train[i], X_train[j], self.sigma)\n",
    "               # print(K[i,j])\n",
    "        X_train=np.asarray(X_train)\n",
    "        K1 = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K1[i,j] = gaussian_kernel(X_train[i], X_train[j], self.sigma)\n",
    "               # print(K[i,j])\n",
    "        #print(K1.shape)\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        #print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a_org=a\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        self.sv_yorg=y\n",
    "        self.kernel = gaussian_kernel\n",
    "        X_train=np.asarray(X_train)\n",
    "        b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            b += self.sv_y[n]\n",
    "            b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        b /= len(self.a)\n",
    "       # print(self.a_org[1])\n",
    "        #print(self.a_org.shape,self.sv_yorg.shape,K.shape)\n",
    "        w_phi=0\n",
    "        total=0\n",
    "        for n in range(len(self.a_org)):\n",
    "            w_phi = self.a_org[n] * self.sv_yorg[n] * K1[n] \n",
    "        self.d_hyp=np.zeros(n_samples)\n",
    "        for n in range(len(self.a_org)):\n",
    "            self.d_hyp += self.sv_yorg[n]*(w_phi+b)\n",
    "        func=np.zeros((n_samples))\n",
    "        func=np.asarray(func)\n",
    "        typ=1\n",
    "        if(typ==1):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=1-(self.d_hyp[i]/(np.amax(self.d_hyp[i])+0.000001))\n",
    "        beta=0.8\n",
    "        if(typ==2):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=2/(1+beta*self.d_hyp[i])\n",
    "        r_max=103/4074\n",
    "        r_min=1\n",
    "        self.m=func[0:115]*r_min\n",
    "        #print(self.m.shape)\n",
    "        self.m=np.append(self.m,func[115:5473]*r_max)\n",
    "        #print(self.m.shape)\n",
    "        \n",
    " ##############################################################################\n",
    "\n",
    "    #prendeva come argomento anche x_test, l'ho tolto, ho aggiunto K\n",
    "    def fit(self, X_train, y):\n",
    "        \n",
    "        self.m_func(X_train, y)\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X_train.shape \n",
    "\n",
    "        # Gram matrix\n",
    "\n",
    "        #print(self.K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        #print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        for i in range(n_samples):\n",
    "            sv=np.logical_or(self.a_org <self.m, self.a_org > 1e-5)\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        #print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "        #print(self.b)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else :\n",
    "            self.w = None\n",
    "            \n",
    "        return self    \n",
    "        \n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv, self.sigma)\n",
    "                    #print(gaussian_kernel(X[i], sv, self.sigma)>0)\n",
    "                y_predict[i] = s\n",
    "                #print(y_predict[i]+self.b)\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2895e+04 -5.5000e+04  8e+04  1e-02  3e-14\n",
      " 1:  2.9955e+03 -4.5212e+03  8e+03  5e-04  3e-14\n",
      " 2:  4.2851e+02 -5.2711e+02  1e+03  1e-07  2e-14\n",
      " 3:  5.4419e+01 -8.3371e+01  1e+02  1e-14  9e-15\n",
      " 4:  4.5604e+00 -1.4868e+01  2e+01  3e-16  4e-15\n",
      " 5: -1.2091e+00 -3.7356e+00  3e+00  2e-16  1e-15\n",
      " 6: -1.6501e+00 -2.4024e+00  8e-01  5e-16  4e-16\n",
      " 7: -1.9095e+00 -2.1761e+00  3e-01  7e-16  3e-16\n",
      " 8: -1.9663e+00 -2.0643e+00  1e-01  3e-16  3e-16\n",
      " 9: -1.9933e+00 -2.0418e+00  5e-02  2e-16  3e-16\n",
      "10: -2.0088e+00 -2.0113e+00  2e-03  4e-16  4e-16\n",
      "11: -2.0099e+00 -2.0099e+00  3e-05  1e-15  4e-16\n",
      "12: -2.0099e+00 -2.0099e+00  3e-07  5e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2895e+04 -5.5000e+04  8e+04  1e-02  3e-14\n",
      " 1:  2.9955e+03 -4.5212e+03  8e+03  5e-04  3e-14\n",
      " 2:  4.2851e+02 -5.2711e+02  1e+03  1e-07  2e-14\n",
      " 3:  5.4419e+01 -8.3371e+01  1e+02  1e-14  9e-15\n",
      " 4:  4.5604e+00 -1.4868e+01  2e+01  3e-16  4e-15\n",
      " 5: -1.2091e+00 -3.7356e+00  3e+00  2e-16  1e-15\n",
      " 6: -1.6501e+00 -2.4024e+00  8e-01  5e-16  4e-16\n",
      " 7: -1.9095e+00 -2.1761e+00  3e-01  7e-16  3e-16\n",
      " 8: -1.9663e+00 -2.0643e+00  1e-01  3e-16  3e-16\n",
      " 9: -1.9933e+00 -2.0418e+00  5e-02  2e-16  3e-16\n",
      "10: -2.0088e+00 -2.0113e+00  2e-03  4e-16  4e-16\n",
      "11: -2.0099e+00 -2.0099e+00  3e-05  1e-15  4e-16\n",
      "12: -2.0099e+00 -2.0099e+00  3e-07  5e-16  4e-16\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.3121e+04 -5.8560e+04  9e+04  2e-02  3e-14\n",
      " 1:  3.2009e+03 -4.7663e+03  8e+03  6e-04  3e-14\n",
      " 2:  4.5973e+02 -5.6315e+02  1e+03  3e-14  3e-14\n",
      " 3:  5.8585e+01 -8.8970e+01  1e+02  1e-14  1e-14\n",
      " 4:  5.0292e+00 -1.5791e+01  2e+01  7e-16  4e-15\n",
      " 5: -1.2038e+00 -3.9215e+00  3e+00  2e-16  1e-15\n",
      " 6: -1.6949e+00 -2.5130e+00  8e-01  1e-15  4e-16\n",
      " 7: -1.9808e+00 -2.3095e+00  3e-01  1e-15  3e-16\n",
      " 8: -2.0521e+00 -2.1777e+00  1e-01  4e-16  3e-16\n",
      " 9: -2.0893e+00 -2.1358e+00  5e-02  7e-16  3e-16\n",
      "10: -2.1050e+00 -2.1161e+00  1e-02  1e-15  3e-16\n",
      "11: -2.1097e+00 -2.1098e+00  1e-04  1e-15  4e-16\n",
      "12: -2.1098e+00 -2.1098e+00  1e-06  7e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.3121e+04 -5.8560e+04  9e+04  2e-02  3e-14\n",
      " 1:  3.2009e+03 -4.7663e+03  8e+03  6e-04  3e-14\n",
      " 2:  4.5973e+02 -5.6315e+02  1e+03  3e-14  3e-14\n",
      " 3:  5.8585e+01 -8.8970e+01  1e+02  1e-14  1e-14\n",
      " 4:  5.0292e+00 -1.5791e+01  2e+01  7e-16  4e-15\n",
      " 5: -1.2038e+00 -3.9215e+00  3e+00  2e-16  1e-15\n",
      " 6: -1.6949e+00 -2.5130e+00  8e-01  1e-15  4e-16\n",
      " 7: -1.9808e+00 -2.3095e+00  3e-01  1e-15  3e-16\n",
      " 8: -2.0521e+00 -2.1777e+00  1e-01  4e-16  3e-16\n",
      " 9: -2.0893e+00 -2.1358e+00  5e-02  7e-16  3e-16\n",
      "10: -2.1050e+00 -2.1161e+00  1e-02  1e-15  3e-16\n",
      "11: -2.1097e+00 -2.1098e+00  1e-04  1e-15  4e-16\n",
      "12: -2.1098e+00 -2.1098e+00  1e-06  7e-16  4e-16\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2959e+04 -5.6532e+04  8e+04  2e-02  3e-14\n",
      " 1:  3.1106e+03 -4.8384e+03  8e+03  6e-04  3e-14\n",
      " 2:  4.5064e+02 -5.5575e+02  1e+03  1e-06  3e-14\n",
      " 3:  5.7401e+01 -8.7436e+01  1e+02  3e-15  1e-14\n",
      " 4:  4.8946e+00 -1.5532e+01  2e+01  1e-15  4e-15\n",
      " 5: -1.2124e+00 -3.8742e+00  3e+00  7e-16  1e-15\n",
      " 6: -1.6887e+00 -2.4707e+00  8e-01  2e-15  5e-16\n",
      " 7: -1.9069e+00 -2.3167e+00  4e-01  2e-16  4e-16\n",
      " 8: -2.0027e+00 -2.1448e+00  1e-01  2e-16  3e-16\n",
      " 9: -2.0429e+00 -2.1132e+00  7e-02  3e-16  3e-16\n",
      "10: -2.0653e+00 -2.0737e+00  8e-03  1e-15  4e-16\n",
      "11: -2.0664e+00 -2.0730e+00  7e-03  2e-16  3e-16\n",
      "12: -2.0693e+00 -2.0695e+00  2e-04  6e-16  4e-16\n",
      "13: -2.0694e+00 -2.0694e+00  2e-06  2e-16  4e-16\n",
      "14: -2.0694e+00 -2.0694e+00  2e-08  7e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2959e+04 -5.6532e+04  8e+04  2e-02  3e-14\n",
      " 1:  3.1106e+03 -4.8384e+03  8e+03  6e-04  3e-14\n",
      " 2:  4.5064e+02 -5.5575e+02  1e+03  1e-06  3e-14\n",
      " 3:  5.7401e+01 -8.7436e+01  1e+02  3e-15  1e-14\n",
      " 4:  4.8946e+00 -1.5532e+01  2e+01  1e-15  4e-15\n",
      " 5: -1.2124e+00 -3.8742e+00  3e+00  7e-16  1e-15\n",
      " 6: -1.6887e+00 -2.4707e+00  8e-01  2e-15  5e-16\n",
      " 7: -1.9069e+00 -2.3167e+00  4e-01  2e-16  4e-16\n",
      " 8: -2.0027e+00 -2.1448e+00  1e-01  2e-16  3e-16\n",
      " 9: -2.0429e+00 -2.1132e+00  7e-02  3e-16  3e-16\n",
      "10: -2.0653e+00 -2.0737e+00  8e-03  1e-15  4e-16\n",
      "11: -2.0664e+00 -2.0730e+00  7e-03  2e-16  3e-16\n",
      "12: -2.0693e+00 -2.0695e+00  2e-04  6e-16  4e-16\n",
      "13: -2.0694e+00 -2.0694e+00  2e-06  2e-16  4e-16\n",
      "14: -2.0694e+00 -2.0694e+00  2e-08  7e-16  4e-16\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.3069e+04 -5.7348e+04  8e+04  2e-02  4e-14\n",
      " 1:  3.1294e+03 -4.6932e+03  8e+03  6e-04  3e-14\n",
      " 2:  4.4875e+02 -5.5072e+02  1e+03  1e-15  3e-14\n",
      " 3:  5.7087e+01 -8.7088e+01  1e+02  1e-14  9e-15\n",
      " 4:  4.8411e+00 -1.5493e+01  2e+01  9e-15  4e-15\n",
      " 5: -1.2227e+00 -3.8714e+00  3e+00  2e-16  1e-15\n",
      " 6: -1.6987e+00 -2.4777e+00  8e-01  2e-16  4e-16\n",
      " 7: -1.9596e+00 -2.3122e+00  4e-01  9e-16  3e-16\n",
      " 8: -2.0323e+00 -2.1643e+00  1e-01  2e-16  3e-16\n",
      " 9: -2.0700e+00 -2.1189e+00  5e-02  7e-16  4e-16\n",
      "10: -2.0866e+00 -2.0973e+00  1e-02  2e-16  4e-16\n",
      "11: -2.0911e+00 -2.0913e+00  1e-04  6e-16  4e-16\n",
      "12: -2.0912e+00 -2.0912e+00  1e-06  6e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.3069e+04 -5.7348e+04  8e+04  2e-02  4e-14\n",
      " 1:  3.1294e+03 -4.6932e+03  8e+03  6e-04  3e-14\n",
      " 2:  4.4875e+02 -5.5072e+02  1e+03  1e-15  3e-14\n",
      " 3:  5.7087e+01 -8.7088e+01  1e+02  1e-14  9e-15\n",
      " 4:  4.8411e+00 -1.5493e+01  2e+01  9e-15  4e-15\n",
      " 5: -1.2227e+00 -3.8714e+00  3e+00  2e-16  1e-15\n",
      " 6: -1.6987e+00 -2.4777e+00  8e-01  2e-16  4e-16\n",
      " 7: -1.9596e+00 -2.3122e+00  4e-01  9e-16  3e-16\n",
      " 8: -2.0323e+00 -2.1643e+00  1e-01  2e-16  3e-16\n",
      " 9: -2.0700e+00 -2.1189e+00  5e-02  7e-16  4e-16\n",
      "10: -2.0866e+00 -2.0973e+00  1e-02  2e-16  4e-16\n",
      "11: -2.0911e+00 -2.0913e+00  1e-04  6e-16  4e-16\n",
      "12: -2.0912e+00 -2.0912e+00  1e-06  6e-16  4e-16\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2771e+04 -5.4063e+04  8e+04  1e-02  3e-14\n",
      " 1:  2.8765e+03 -3.9667e+03  7e+03  4e-04  4e-14\n",
      " 2:  4.0681e+02 -5.0177e+02  9e+02  3e-14  2e-14\n",
      " 3:  5.1506e+01 -7.9222e+01  1e+02  2e-15  9e-15\n",
      " 4:  4.2438e+00 -1.4198e+01  2e+01  4e-15  4e-15\n",
      " 5: -1.1957e+00 -3.5911e+00  2e+00  5e-16  1e-15\n",
      " 6: -1.5927e+00 -2.2819e+00  7e-01  1e-15  4e-16\n",
      " 7: -1.8470e+00 -2.0615e+00  2e-01  5e-16  3e-16\n",
      " 8: -1.8911e+00 -1.9994e+00  1e-01  9e-16  3e-16\n",
      " 9: -1.9200e+00 -1.9356e+00  2e-02  9e-16  4e-16\n",
      "10: -1.9239e+00 -1.9315e+00  8e-03  1e-15  4e-16\n",
      "11: -1.9267e+00 -1.9269e+00  1e-04  1e-15  4e-16\n",
      "12: -1.9268e+00 -1.9268e+00  2e-06  2e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2771e+04 -5.4063e+04  8e+04  1e-02  3e-14\n",
      " 1:  2.8765e+03 -3.9667e+03  7e+03  4e-04  4e-14\n",
      " 2:  4.0681e+02 -5.0177e+02  9e+02  3e-14  2e-14\n",
      " 3:  5.1506e+01 -7.9222e+01  1e+02  2e-15  9e-15\n",
      " 4:  4.2438e+00 -1.4198e+01  2e+01  4e-15  4e-15\n",
      " 5: -1.1957e+00 -3.5911e+00  2e+00  5e-16  1e-15\n",
      " 6: -1.5927e+00 -2.2819e+00  7e-01  1e-15  4e-16\n",
      " 7: -1.8470e+00 -2.0615e+00  2e-01  5e-16  3e-16\n",
      " 8: -1.8911e+00 -1.9994e+00  1e-01  9e-16  3e-16\n",
      " 9: -1.9200e+00 -1.9356e+00  2e-02  9e-16  4e-16\n",
      "10: -1.9239e+00 -1.9315e+00  8e-03  1e-15  4e-16\n",
      "11: -1.9267e+00 -1.9269e+00  1e-04  1e-15  4e-16\n",
      "12: -1.9268e+00 -1.9268e+00  2e-06  2e-16  4e-16\n",
      "Optimal solution found.\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def hyp_svm():\n",
    "        C_vals = [1e-2, 1e-1, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "        sigma = [9e-2, 9e-1, 9e+1, 9e+2, 9e+3, 9e+4]\n",
    "        combs = list(itertools.product(C_vals, sigma))\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            typ = 2            \n",
    "            clf = HYP_SVM(C=100.0, kernel=gaussian_kernel, sigma=0.9)    \n",
    "    \n",
    "            #clf.m_func(X_train, y_train)\n",
    "                \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_predict = clf.predict(X_test)\n",
    "\n",
    "            clf.score(y_predict,y_test)\n",
    "            #print(y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "            \n",
    "            \n",
    "    hyp_svm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_svm = svm.SVC(kernel='rbf', gamma=0.001, C=100)\n",
    "#clf_svm.fit(X_train, y_train)\n",
    "#y_pred_svm = clf_svm.predict(X_test) \n",
    "#acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "#print (\"Overall RBF KERNEL SVM accuracy: \",acc_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = [1e-2, 1e-1, 1, 1e+1, 1e+2, 1e+3, 1e+4]\n",
    "sigma = [9e-2, 9e-1, 9, 9e+1, 9e+2, 9e+3, 9e+4]\n",
    "#kernels = [\"linear_kernel\", \"polynomial_kernel\", \"gaussian_kernel\"]\n",
    "parameters = {'C': C_vals, 'sigma': sigma}\n",
    "\n",
    "model = HYP_SVM(C=100, kernel=gaussian_kernel, sigma=0.9)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=30)\n",
    "\n",
    "clf = GridSearchCV(model, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C_vals = np.array(C_vals)\n",
    "#sigma = np.array(sigma)\n",
    "#C_vals=C_vals.astype(float)\n",
    "#sigma=sigma.astype(float)\n",
    "#clf = GridSearchCV(model, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0005e+01 -5.7511e+00  4e+02  2e+01  3e-16\n",
      " 1: -4.7652e+00 -2.9038e+00  3e+01  2e+00  4e-16\n",
      " 2: -5.7902e-01 -1.8697e+00  1e+00  4e-03  1e-15\n",
      " 3: -6.4741e-01 -7.5227e-01  1e-01  3e-04  5e-16\n",
      " 4: -6.6302e-01 -6.6974e-01  7e-03  2e-05  4e-16\n",
      " 5: -6.6453e-01 -6.6579e-01  1e-03  2e-06  4e-16\n",
      " 6: -6.6482e-01 -6.6506e-01  2e-04  3e-07  4e-16\n",
      " 7: -6.6488e-01 -6.6491e-01  3e-05  2e-08  4e-16\n",
      " 8: -6.6489e-01 -6.6489e-01  2e-06  6e-10  4e-16\n",
      " 9: -6.6489e-01 -6.6489e-01  1e-07  1e-11  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0005e+01 -5.7511e+00  4e+02  2e+01  3e-16\n",
      " 1: -4.7652e+00 -2.9038e+00  3e+01  2e+00  4e-16\n",
      " 2: -5.7902e-01 -1.8697e+00  1e+00  4e-03  1e-15\n",
      " 3: -6.4741e-01 -7.5227e-01  1e-01  3e-04  5e-16\n",
      " 4: -6.6302e-01 -6.6974e-01  7e-03  2e-05  4e-16\n",
      " 5: -6.6453e-01 -6.6579e-01  1e-03  2e-06  4e-16\n",
      " 6: -6.6482e-01 -6.6506e-01  2e-04  3e-07  4e-16\n",
      " 7: -6.6488e-01 -6.6491e-01  3e-05  2e-08  4e-16\n",
      " 8: -6.6489e-01 -6.6489e-01  2e-06  6e-10  4e-16\n",
      " 9: -6.6489e-01 -6.6489e-01  1e-07  1e-11  4e-16\n",
      "Optimal solution found.\n",
      "[[5.3 2.3]\n",
      " [5.3 1.9]\n",
      " [3.7 1. ]\n",
      " [5.1 1.6]\n",
      " [5.4 2.1]\n",
      " [3.5 1. ]\n",
      " [1.5 0.2]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [4.7 1.5]\n",
      " [1.3 0.2]\n",
      " [4.9 1.5]\n",
      " [1.5 0.1]\n",
      " [6.1 2.5]\n",
      " [4.1 1. ]\n",
      " [1.5 0.3]\n",
      " [3.6 1.3]\n",
      " [6.7 2. ]\n",
      " [1.2 0.2]\n",
      " [5.1 2.3]\n",
      " [5.9 2.1]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [4.5 1.5]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e316bf9d1d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clf.score(xTest, yTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#clf.best_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                       *args, **kwargs)\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-312d389a752a>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, y_predict, y_test)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(\"y_test max\",test_max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mpred_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_min\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "grid_result = clf.fit(X=xTrain, y=yTrain)\n",
    "\n",
    "#clf.score(xTest, yTest)\n",
    "#clf.best_params_\n",
    "\n",
    "#best_model = FuzzyMMC(sensitivity=0.5, exp_bound=0.60, animate=False)\n",
    "#best_model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=gaussian_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "    def fit(self, X, y):\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X.shape\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = gaussian_kernel(X[i], X[j])\n",
    "               # print(K[i,j])\n",
    "        print(K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "       # print(a)\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "                #print(self.w)\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "              #  print(y_predict[i])\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def normal_svm():\n",
    "        \n",
    "        clf = SVM(C=100.0)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)    \n",
    "            y_predict = clf.predict(X_test)\n",
    "            gm(y_predict,y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "    normal_svm()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
