{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "from sklearn import svm\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "dataset=pd.read_csv(\"iris-setosa.csv\")\n",
    "\n",
    "X = dataset.columns[1:3]\n",
    "X = dataset[X]\n",
    "y = dataset.columns[0]\n",
    "y = dataset[y]\n",
    "\n",
    "#y Ã¨ array di array, io voglio array!\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X=X.astype(float)\n",
    "Y=y.astype(float)\n",
    "y=np.where(y==0,-1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "#for train_index, test_index in skf.split(X, y):\n",
    "#    #print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "#    X_train, X_test = X[train_index], X[test_index]\n",
    "#    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=90.0):\n",
    "   # print(-linalg.norm(x-y)**2)\n",
    "    x=np.asarray(x)\n",
    "    y=np.asarray(y)\n",
    "    return np.exp((-linalg.norm(x-y)**2) / (2 * (sigma ** 2)))\n",
    "\n",
    "def gm(y_predict,y_test):\n",
    "    test_min=0\n",
    "    test_max=0\n",
    "    pred_min=0\n",
    "    pred_max=0\n",
    "    y_test=np.asarray(y_test)\n",
    "    for i in range(0,len(y_test)):\n",
    "        if(y_test[i]==1):\n",
    "             test_min=test_min+1\n",
    "        else:\n",
    "             test_max=test_max+1\n",
    "    print(\"y_test min\",test_min)       \n",
    "    print(\"y_test max\",test_max)\n",
    "    for i in range(0,len(y_predict)):\n",
    "        if(y_predict[i]==1 and y_predict[i]==y_test[i]):\n",
    "             pred_min=pred_min+1\n",
    "        elif(y_predict[i]==-1 and y_predict[i]==y_test[i]):\n",
    "             pred_max=pred_max+1\n",
    "    print(\"y_pred min\",pred_min)       \n",
    "    print(\"y_pred max\",pred_max)\n",
    "    se=pred_min/test_min\n",
    "    sp=pred_max/test_max\n",
    "    print(se,sp)\n",
    "    gm=math.sqrt(se*sp)\n",
    "    print(\"GM\",gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class HYP_SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=gaussian_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "    def m_func(self, X_train,X_test, y):\n",
    "        n_samples, n_features = X_train.shape \n",
    "        nt_samples, nt_features= X_test.shape\n",
    "        self.K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                self.K[i,j] = gaussian_kernel(X_train[i], X_train[j])\n",
    "               # print(K[i,j])\n",
    "        X_train=np.asarray(X_train)\n",
    "        X_test=np.asarray(X_test)\n",
    "        K1 = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K1[i,j] = gaussian_kernel(X_train[i], X_train[j])\n",
    "               # print(K[i,j])\n",
    "        print(K1.shape)\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a_org=a\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        self.sv_yorg=y\n",
    "        self.kernel = gaussian_kernel\n",
    "        X_train=np.asarray(X_train)\n",
    "        b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            b += self.sv_y[n]\n",
    "            b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        b /= len(self.a)\n",
    "       # print(self.a_org[1])\n",
    "        #print(self.a_org.shape,self.sv_yorg.shape,K.shape)\n",
    "        w_phi=0\n",
    "        total=0\n",
    "        for n in range(len(self.a_org)):\n",
    "            w_phi = self.a_org[n] * self.sv_yorg[n] * K1[n] \n",
    "        self.d_hyp=np.zeros(n_samples)\n",
    "        for n in range(len(self.a_org)):\n",
    "            self.d_hyp += self.sv_yorg[n]*(w_phi+b)\n",
    "        func=np.zeros((n_samples))\n",
    "        func=np.asarray(func)\n",
    "        typ=1\n",
    "        if(typ==1):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=1-(self.d_hyp[i]/(np.amax(self.d_hyp[i])+0.000001))\n",
    "        beta=0.8\n",
    "        if(typ==2):\n",
    "            for i in range(n_samples):\n",
    "                func[i]=2/(1+beta*self.d_hyp[i])\n",
    "        r_max=103/4074\n",
    "        r_min=1\n",
    "        self.m=func[0:115]*r_min\n",
    "        print(self.m.shape)\n",
    "        self.m=np.append(self.m,func[115:5473]*r_max)\n",
    "        print(self.m.shape)\n",
    "        \n",
    " ##############################################################################\n",
    "\n",
    "\n",
    "    def fit(self, X_train,X_test, y):\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X_train.shape \n",
    "        nt_samples, nt_features = X_test.shape\n",
    "        # Gram matrix\n",
    "\n",
    "        print(self.K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "        a_org = np.ravel(solution['x'])\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        for i in range(n_samples):\n",
    "            sv=np.logical_or(self.a_org <self.m, self.a_org > 1e-5)\n",
    "        #print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X_train[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        #print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * self.K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "        print(self.b)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else :\n",
    "            self.w = None        \n",
    "        \n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "              #  print(y_predict[i])\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.4850e+03 -1.5659e+05  2e+05  6e-13  3e-13\n",
      " 1: -4.5794e+03 -1.0051e+04  5e+03  2e-12  5e-13\n",
      " 2: -5.3482e+03 -9.2682e+03  4e+03  4e-13  4e-13\n",
      " 3: -6.0886e+03 -8.3477e+03  2e+03  8e-13  5e-13\n",
      " 4: -6.6199e+03 -7.4566e+03  8e+02  2e-13  6e-13\n",
      " 5: -6.8005e+03 -7.2157e+03  4e+02  7e-13  5e-13\n",
      " 6: -6.8692e+03 -7.1261e+03  3e+02  2e-12  5e-13\n",
      " 7: -6.9268e+03 -7.0556e+03  1e+02  4e-13  5e-13\n",
      " 8: -6.9595e+03 -7.0159e+03  6e+01  2e-16  6e-13\n",
      " 9: -6.9812e+03 -6.9904e+03  9e+00  2e-12  7e-13\n",
      "10: -6.9855e+03 -6.9856e+03  1e-01  2e-12  7e-13\n",
      "11: -6.9855e+03 -6.9855e+03  1e-03  2e-12  7e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.4850e+03 -1.5659e+05  2e+05  6e-13  3e-13\n",
      " 1: -4.5794e+03 -1.0051e+04  5e+03  2e-12  5e-13\n",
      " 2: -5.3482e+03 -9.2682e+03  4e+03  4e-13  4e-13\n",
      " 3: -6.0886e+03 -8.3477e+03  2e+03  8e-13  5e-13\n",
      " 4: -6.6199e+03 -7.4566e+03  8e+02  2e-13  6e-13\n",
      " 5: -6.8005e+03 -7.2157e+03  4e+02  7e-13  5e-13\n",
      " 6: -6.8692e+03 -7.1261e+03  3e+02  2e-12  5e-13\n",
      " 7: -6.9268e+03 -7.0556e+03  1e+02  4e-13  5e-13\n",
      " 8: -6.9595e+03 -7.0159e+03  6e+01  2e-16  6e-13\n",
      " 9: -6.9812e+03 -6.9904e+03  9e+00  2e-12  7e-13\n",
      "10: -6.9855e+03 -6.9856e+03  1e-01  2e-12  7e-13\n",
      "11: -6.9855e+03 -6.9855e+03  1e-03  2e-12  7e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "-0.1302299754616456\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 7\n",
      "y_pred max 20\n",
      "0.7 1.0\n",
      "GM 0.8366600265340756\n",
      "27 out of 30 predictions correct\n",
      "Accuracy 0.9\n",
      "Errore quadratico medio:  0.4\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6173e+03 -1.5693e+05  2e+05  8e-13  4e-13\n",
      " 1: -4.7228e+03 -1.0036e+04  5e+03  1e-13  5e-13\n",
      " 2: -5.6545e+03 -9.1640e+03  4e+03  8e-13  5e-13\n",
      " 3: -6.4361e+03 -8.2122e+03  2e+03  1e-12  6e-13\n",
      " 4: -6.9424e+03 -7.4310e+03  5e+02  2e-12  6e-13\n",
      " 5: -7.0664e+03 -7.2984e+03  2e+02  1e-12  5e-13\n",
      " 6: -7.1134e+03 -7.2424e+03  1e+02  2e-13  5e-13\n",
      " 7: -7.1586e+03 -7.1908e+03  3e+01  2e-13  6e-13\n",
      " 8: -7.1737e+03 -7.1758e+03  2e+00  1e-13  6e-13\n",
      " 9: -7.1747e+03 -7.1748e+03  3e-02  2e-12  6e-13\n",
      "10: -7.1748e+03 -7.1748e+03  3e-04  8e-13  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6173e+03 -1.5693e+05  2e+05  8e-13  4e-13\n",
      " 1: -4.7228e+03 -1.0036e+04  5e+03  1e-13  5e-13\n",
      " 2: -5.6545e+03 -9.1640e+03  4e+03  8e-13  5e-13\n",
      " 3: -6.4361e+03 -8.2122e+03  2e+03  1e-12  6e-13\n",
      " 4: -6.9424e+03 -7.4310e+03  5e+02  2e-12  6e-13\n",
      " 5: -7.0664e+03 -7.2984e+03  2e+02  1e-12  5e-13\n",
      " 6: -7.1134e+03 -7.2424e+03  1e+02  2e-13  5e-13\n",
      " 7: -7.1586e+03 -7.1908e+03  3e+01  2e-13  6e-13\n",
      " 8: -7.1737e+03 -7.1758e+03  2e+00  1e-13  6e-13\n",
      " 9: -7.1747e+03 -7.1748e+03  3e-02  2e-12  6e-13\n",
      "10: -7.1748e+03 -7.1748e+03  3e-04  8e-13  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "-0.10298047134696968\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 9\n",
      "y_pred max 20\n",
      "0.9 1.0\n",
      "GM 0.9486832980505138\n",
      "29 out of 30 predictions correct\n",
      "Accuracy 0.9666666666666667\n",
      "Errore quadratico medio:  0.13333333333333333\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5150e+03 -1.5667e+05  2e+05  1e-12  4e-13\n",
      " 1: -4.6113e+03 -1.0026e+04  5e+03  1e-12  5e-13\n",
      " 2: -5.3796e+03 -9.2343e+03  4e+03  1e-12  5e-13\n",
      " 3: -6.0975e+03 -8.3550e+03  2e+03  9e-13  5e-13\n",
      " 4: -6.7144e+03 -7.3277e+03  6e+02  2e-12  6e-13\n",
      " 5: -6.8649e+03 -7.1671e+03  3e+02  1e-14  6e-13\n",
      " 6: -6.9300e+03 -7.0923e+03  2e+02  9e-13  5e-13\n",
      " 7: -6.9734e+03 -7.0463e+03  7e+01  1e-12  6e-13\n",
      " 8: -6.9828e+03 -7.0328e+03  5e+01  1e-12  6e-13\n",
      " 9: -7.0043e+03 -7.0109e+03  7e+00  7e-14  6e-13\n",
      "10: -7.0074e+03 -7.0076e+03  2e-01  2e-12  6e-13\n",
      "11: -7.0075e+03 -7.0075e+03  3e-03  2e-12  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5150e+03 -1.5667e+05  2e+05  1e-12  4e-13\n",
      " 1: -4.6113e+03 -1.0026e+04  5e+03  1e-12  5e-13\n",
      " 2: -5.3796e+03 -9.2343e+03  4e+03  1e-12  5e-13\n",
      " 3: -6.0975e+03 -8.3550e+03  2e+03  9e-13  5e-13\n",
      " 4: -6.7144e+03 -7.3277e+03  6e+02  2e-12  6e-13\n",
      " 5: -6.8649e+03 -7.1671e+03  3e+02  1e-14  6e-13\n",
      " 6: -6.9300e+03 -7.0923e+03  2e+02  9e-13  5e-13\n",
      " 7: -6.9734e+03 -7.0463e+03  7e+01  1e-12  6e-13\n",
      " 8: -6.9828e+03 -7.0328e+03  5e+01  1e-12  6e-13\n",
      " 9: -7.0043e+03 -7.0109e+03  7e+00  7e-14  6e-13\n",
      "10: -7.0074e+03 -7.0076e+03  2e-01  2e-12  6e-13\n",
      "11: -7.0075e+03 -7.0075e+03  3e-03  2e-12  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "-0.11509943680161049\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 9\n",
      "y_pred max 20\n",
      "0.9 1.0\n",
      "GM 0.9486832980505138\n",
      "29 out of 30 predictions correct\n",
      "Accuracy 0.9666666666666667\n",
      "Errore quadratico medio:  0.13333333333333333\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5452e+03 -1.5664e+05  2e+05  6e-13  4e-13\n",
      " 1: -4.6450e+03 -9.9281e+03  5e+03  2e-13  6e-13\n",
      " 2: -5.5602e+03 -9.0261e+03  3e+03  1e-12  5e-13\n",
      " 3: -6.2771e+03 -8.1721e+03  2e+03  3e-13  6e-13\n",
      " 4: -6.7579e+03 -7.4609e+03  7e+02  9e-13  6e-13\n",
      " 5: -6.9355e+03 -7.2503e+03  3e+02  3e-13  5e-13\n",
      " 6: -7.0331e+03 -7.1331e+03  1e+02  3e-13  6e-13\n",
      " 7: -7.0709e+03 -7.0942e+03  2e+01  1e-12  6e-13\n",
      " 8: -7.0820e+03 -7.0833e+03  1e+00  2e-12  6e-13\n",
      " 9: -7.0827e+03 -7.0827e+03  1e-02  2e-12  6e-13\n",
      "10: -7.0827e+03 -7.0827e+03  1e-04  7e-14  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5452e+03 -1.5664e+05  2e+05  6e-13  4e-13\n",
      " 1: -4.6450e+03 -9.9281e+03  5e+03  2e-13  6e-13\n",
      " 2: -5.5602e+03 -9.0261e+03  3e+03  1e-12  5e-13\n",
      " 3: -6.2771e+03 -8.1721e+03  2e+03  3e-13  6e-13\n",
      " 4: -6.7579e+03 -7.4609e+03  7e+02  9e-13  6e-13\n",
      " 5: -6.9355e+03 -7.2503e+03  3e+02  3e-13  5e-13\n",
      " 6: -7.0331e+03 -7.1331e+03  1e+02  3e-13  6e-13\n",
      " 7: -7.0709e+03 -7.0942e+03  2e+01  1e-12  6e-13\n",
      " 8: -7.0820e+03 -7.0833e+03  1e+00  2e-12  6e-13\n",
      " 9: -7.0827e+03 -7.0827e+03  1e-02  2e-12  6e-13\n",
      "10: -7.0827e+03 -7.0827e+03  1e-04  7e-14  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "-0.027382030361559038\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5243e+03 -1.5666e+05  2e+05  7e-13  4e-13\n",
      " 1: -4.6227e+03 -1.0090e+04  5e+03  7e-14  5e-13\n",
      " 2: -5.4597e+03 -9.2896e+03  4e+03  1e-12  4e-13\n",
      " 3: -6.2064e+03 -8.3568e+03  2e+03  7e-13  5e-13\n",
      " 4: -6.7623e+03 -7.4637e+03  7e+02  2e-13  6e-13\n",
      " 5: -6.9236e+03 -7.2603e+03  3e+02  6e-13  5e-13\n",
      " 6: -7.0089e+03 -7.1638e+03  2e+02  2e-12  6e-13\n",
      " 7: -7.0487e+03 -7.1160e+03  7e+01  6e-13  6e-13\n",
      " 8: -7.0684e+03 -7.0939e+03  3e+01  7e-13  5e-13\n",
      " 9: -7.0802e+03 -7.0813e+03  1e+00  3e-13  7e-13\n",
      "10: -7.0807e+03 -7.0807e+03  1e-02  1e-12  7e-13\n",
      "11: -7.0807e+03 -7.0807e+03  1e-04  2e-12  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(115,)\n",
      "(120,)\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5243e+03 -1.5666e+05  2e+05  7e-13  4e-13\n",
      " 1: -4.6227e+03 -1.0090e+04  5e+03  7e-14  5e-13\n",
      " 2: -5.4597e+03 -9.2896e+03  4e+03  1e-12  4e-13\n",
      " 3: -6.2064e+03 -8.3568e+03  2e+03  7e-13  5e-13\n",
      " 4: -6.7623e+03 -7.4637e+03  7e+02  2e-13  6e-13\n",
      " 5: -6.9236e+03 -7.2603e+03  3e+02  6e-13  5e-13\n",
      " 6: -7.0089e+03 -7.1638e+03  2e+02  2e-12  6e-13\n",
      " 7: -7.0487e+03 -7.1160e+03  7e+01  6e-13  6e-13\n",
      " 8: -7.0684e+03 -7.0939e+03  3e+01  7e-13  5e-13\n",
      " 9: -7.0802e+03 -7.0813e+03  1e+00  3e-13  7e-13\n",
      "10: -7.0807e+03 -7.0807e+03  1e-02  1e-12  7e-13\n",
      "11: -7.0807e+03 -7.0807e+03  1e-04  2e-12  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "-0.0214332606264735\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def hyp_svm():\n",
    "        \n",
    "        clf = HYP_SVM(C=100.0)\n",
    "        typ=2\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        \n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "            clf.m_func(X_train,X_test,y_train)\n",
    "            clf.fit(X_train,X_test, y_train)\n",
    "            y_predict = clf.predict(X_test)\n",
    "            gm(y_predict,y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "            \n",
    "    hyp_svm()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall RBF KERNEL SVM accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "clf_svm = svm.SVC(kernel='rbf', gamma=0.001, C=100)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_pred_svm = clf_svm.predict(X_test) \n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print (\"Overall RBF KERNEL SVM accuracy: \",acc_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=gaussian_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "    def fit(self, X, y):\n",
    "        self.kernel = gaussian_kernel\n",
    "        n_samples, n_features = X.shape\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = gaussian_kernel(X[i], X[j])\n",
    "               # print(K[i,j])\n",
    "        print(K.shape)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        A = matrix(A, (1,n_samples), 'd') #changes done\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        #print(P,q,A,b)\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "            \n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        print(solution['status'])\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "       # print(a)\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        print(sv.shape)\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == gaussian_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "                #print(self.w)\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            X=np.asarray(X)\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * gaussian_kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "              #  print(y_predict[i])\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.4850e+03 -1.5659e+05  2e+05  6e-13  3e-13\n",
      " 1: -4.5794e+03 -1.0051e+04  5e+03  2e-12  5e-13\n",
      " 2: -5.3482e+03 -9.2682e+03  4e+03  4e-13  4e-13\n",
      " 3: -6.0886e+03 -8.3477e+03  2e+03  8e-13  5e-13\n",
      " 4: -6.6199e+03 -7.4566e+03  8e+02  2e-13  6e-13\n",
      " 5: -6.8005e+03 -7.2157e+03  4e+02  7e-13  5e-13\n",
      " 6: -6.8692e+03 -7.1261e+03  3e+02  2e-12  5e-13\n",
      " 7: -6.9268e+03 -7.0556e+03  1e+02  4e-13  5e-13\n",
      " 8: -6.9595e+03 -7.0159e+03  6e+01  2e-16  6e-13\n",
      " 9: -6.9812e+03 -6.9904e+03  9e+00  2e-12  7e-13\n",
      "10: -6.9855e+03 -6.9856e+03  1e-01  2e-12  7e-13\n",
      "11: -6.9855e+03 -6.9855e+03  1e-03  2e-12  7e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(120,)\n",
      "118 support vectors out of 120 points\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 7\n",
      "y_pred max 20\n",
      "0.7 1.0\n",
      "GM 0.8366600265340756\n",
      "27 out of 30 predictions correct\n",
      "Accuracy 0.9\n",
      "Errore quadratico medio:  0.4\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6173e+03 -1.5693e+05  2e+05  8e-13  4e-13\n",
      " 1: -4.7228e+03 -1.0036e+04  5e+03  1e-13  5e-13\n",
      " 2: -5.6545e+03 -9.1640e+03  4e+03  8e-13  5e-13\n",
      " 3: -6.4361e+03 -8.2122e+03  2e+03  1e-12  6e-13\n",
      " 4: -6.9424e+03 -7.4310e+03  5e+02  2e-12  6e-13\n",
      " 5: -7.0664e+03 -7.2984e+03  2e+02  1e-12  5e-13\n",
      " 6: -7.1134e+03 -7.2424e+03  1e+02  2e-13  5e-13\n",
      " 7: -7.1586e+03 -7.1908e+03  3e+01  2e-13  6e-13\n",
      " 8: -7.1737e+03 -7.1758e+03  2e+00  1e-13  6e-13\n",
      " 9: -7.1747e+03 -7.1748e+03  3e-02  2e-12  6e-13\n",
      "10: -7.1748e+03 -7.1748e+03  3e-04  8e-13  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(120,)\n",
      "96 support vectors out of 120 points\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 9\n",
      "y_pred max 20\n",
      "0.9 1.0\n",
      "GM 0.9486832980505138\n",
      "29 out of 30 predictions correct\n",
      "Accuracy 0.9666666666666667\n",
      "Errore quadratico medio:  0.13333333333333333\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5150e+03 -1.5667e+05  2e+05  1e-12  4e-13\n",
      " 1: -4.6113e+03 -1.0026e+04  5e+03  1e-12  5e-13\n",
      " 2: -5.3796e+03 -9.2343e+03  4e+03  1e-12  5e-13\n",
      " 3: -6.0975e+03 -8.3550e+03  2e+03  9e-13  5e-13\n",
      " 4: -6.7144e+03 -7.3277e+03  6e+02  2e-12  6e-13\n",
      " 5: -6.8649e+03 -7.1671e+03  3e+02  1e-14  6e-13\n",
      " 6: -6.9300e+03 -7.0923e+03  2e+02  9e-13  5e-13\n",
      " 7: -6.9734e+03 -7.0463e+03  7e+01  1e-12  6e-13\n",
      " 8: -6.9828e+03 -7.0328e+03  5e+01  1e-12  6e-13\n",
      " 9: -7.0043e+03 -7.0109e+03  7e+00  7e-14  6e-13\n",
      "10: -7.0074e+03 -7.0076e+03  2e-01  2e-12  6e-13\n",
      "11: -7.0075e+03 -7.0075e+03  3e-03  2e-12  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(120,)\n",
      "120 support vectors out of 120 points\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 9\n",
      "y_pred max 20\n",
      "0.9 1.0\n",
      "GM 0.9486832980505138\n",
      "29 out of 30 predictions correct\n",
      "Accuracy 0.9666666666666667\n",
      "Errore quadratico medio:  0.13333333333333333\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5452e+03 -1.5664e+05  2e+05  6e-13  4e-13\n",
      " 1: -4.6450e+03 -9.9281e+03  5e+03  2e-13  6e-13\n",
      " 2: -5.5602e+03 -9.0261e+03  3e+03  1e-12  5e-13\n",
      " 3: -6.2771e+03 -8.1721e+03  2e+03  3e-13  6e-13\n",
      " 4: -6.7579e+03 -7.4609e+03  7e+02  9e-13  6e-13\n",
      " 5: -6.9355e+03 -7.2503e+03  3e+02  3e-13  5e-13\n",
      " 6: -7.0331e+03 -7.1331e+03  1e+02  3e-13  6e-13\n",
      " 7: -7.0709e+03 -7.0942e+03  2e+01  1e-12  6e-13\n",
      " 8: -7.0820e+03 -7.0833e+03  1e+00  2e-12  6e-13\n",
      " 9: -7.0827e+03 -7.0827e+03  1e-02  2e-12  6e-13\n",
      "10: -7.0827e+03 -7.0827e+03  1e-04  7e-14  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(120,)\n",
      "85 support vectors out of 120 points\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n",
      "(120, 120)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5243e+03 -1.5666e+05  2e+05  7e-13  4e-13\n",
      " 1: -4.6227e+03 -1.0090e+04  5e+03  7e-14  5e-13\n",
      " 2: -5.4597e+03 -9.2896e+03  4e+03  1e-12  4e-13\n",
      " 3: -6.2064e+03 -8.3568e+03  2e+03  7e-13  5e-13\n",
      " 4: -6.7623e+03 -7.4637e+03  7e+02  2e-13  6e-13\n",
      " 5: -6.9236e+03 -7.2603e+03  3e+02  6e-13  5e-13\n",
      " 6: -7.0089e+03 -7.1638e+03  2e+02  2e-12  6e-13\n",
      " 7: -7.0487e+03 -7.1160e+03  7e+01  6e-13  6e-13\n",
      " 8: -7.0684e+03 -7.0939e+03  3e+01  7e-13  5e-13\n",
      " 9: -7.0802e+03 -7.0813e+03  1e+00  3e-13  7e-13\n",
      "10: -7.0807e+03 -7.0807e+03  1e-02  1e-12  7e-13\n",
      "11: -7.0807e+03 -7.0807e+03  1e-04  2e-12  6e-13\n",
      "Optimal solution found.\n",
      "optimal\n",
      "(120,)\n",
      "82 support vectors out of 120 points\n",
      "y_test min 10\n",
      "y_test max 20\n",
      "y_pred min 10\n",
      "y_pred max 20\n",
      "1.0 1.0\n",
      "GM 1.0\n",
      "30 out of 30 predictions correct\n",
      "Accuracy 1.0\n",
      "Errore quadratico medio:  0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl           \n",
    "    def normal_svm():\n",
    "        \n",
    "        clf = SVM(C=100.0)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)    \n",
    "            y_predict = clf.predict(X_test)\n",
    "            gm(y_predict,y_test)\n",
    "            correct = np.sum(y_predict == y_test)\n",
    "            mse = mean_squared_error(y_test, y_predict)\n",
    "            print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "            print(\"Accuracy\",correct/len(y_predict))\n",
    "            print(\"Errore quadratico medio: \", mse)\n",
    "    normal_svm()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
